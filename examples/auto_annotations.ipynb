{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839ee94d-5ead-46da-bd60-5b11e4aaebc4",
   "metadata": {},
   "source": [
    "# Auto Annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ee20f0-007c-4f6a-af1c-ad39b959984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yanndubois/Desktop/GitHub/alpaca_farm\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507e6327-cc7e-4135-b462-457a5e71e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_farm.utils import jload\n",
    "from alpaca_farm.auto_annotations.pairwise_annotators import PairwiseAutoAnnotator, SinglePairwiseAutoAnnotator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8222f-c017-4aae-9b29-cc8210e946fe",
   "metadata": {},
   "source": [
    "The key object that you need for automatic annotations (both for training or for eval) is our `PairwiseAutoAnnotator`. By default the annotator is the pool from Alpaca Farm, for simplicity let's use a single annotator `test` for now.\n",
    "\n",
    "For details about annotators including how to extend them, refer to the [Configuring annotators](#Configuring-annotators) section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d6dcf4-b0fb-4c56-a1fd-1db509af05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = PairwiseAutoAnnotator(annotators_config=\"annotators/test/configs.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99ac04-0d48-410e-95fd-cf3b1360fc4e",
   "metadata": {},
   "source": [
    "## Annotating paired outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef99a2c-77ef-41e6-9db8-eae091afd952",
   "metadata": {},
   "source": [
    "Now let's annotate some pairwise preference. All we need is some data. The annotator takes in either list of dictionaries, or pandas dataframes. The keys of dictionaries (or columns in dataframe) need to always contain `instruction` and `input` which defines the instruction. The annotator also needs a pair of outputs to compare, if you have a sequence of such pairs under the keys `output_1` and `output_2` then you can directly use `annotate_pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ab42c1d-3bda-4797-a1bd-877b958e8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of paired output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Hey everyone! \\n\\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \\n\\nLet me know if you can make it - I'd love to see you all there!\\n\\nCheers,\\n[Your Name]\"}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_pairs = jload(\"examples/data/outputs_pairs.json\")[:6]\n",
    "print(\"Example of paired output:\\n\")\n",
    "outputs_pairs[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67caa934-5a37-429a-affc-dbd182be7c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = annotator.annotate_pairs(outputs_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dd953df-7eb1-4446-86a9-f3b3fbfba665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Hey everyone! \\n\\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \\n\\nLet me know if you can make it - I'd love to see you all there!\\n\\nCheers,\\n[Your Name]\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 2.0}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648a4b0-5d19-4bc7-b63e-d4cb4ab4fd51",
   "metadata": {},
   "source": [
    "Here we see that the annotations adds two keys:\n",
    "- `'preference'`: the index of the preferred output, here `preference=2.0` so `output_2` is prefered\n",
    "- `'annotator'`: the name of the simulated annotator as found in the `annotators_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ee339-cf40-4a3c-a861-19c265f962bc",
   "metadata": {},
   "source": [
    "`annotate_pairs` is the main function and should be used if you have paired outputs to annotate. In many usecases, however, you  will outputs in different formats. In the following we discuss two additional helper function `annotate_head2head` and `annotate_samples` which are paticularly well suited for the typical format during evaluation and training respectively. Both functions call `annotate_pairs` under the hood after a reformatting step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b41af4-731c-4d12-83b4-9b69f4b2bc3f",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "For evaluation we need two components:\n",
    "- outputs from the model we want to compare\n",
    "- outputs on the same examples from the baseline model\n",
    "\n",
    "Often case both of those components will be in a different list of dictionary (one list for each model). In this case all dictionaries need to contain an `output` column. Let us load such data from our simulated RLHF model and text-davinci-003 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2a9cce3-0fe9-4dc2-9b49-de59f641e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of baseline output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_baseline = jload(\"examples/data/outputs_baseline.json\")[:6]\n",
    "print(\"Example of baseline output:\\n\")\n",
    "outputs_baseline[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f808315-da5c-4af3-a026-64e3d07801f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of rlhf output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output': \"Dear Friends,\\n\\nI'm planning a dinner party this Friday and I'd love for you to join me! It's going to be a casual night of great food, interesting conversations, and lots of laughs.\\n\\nI'm looking forward to seeing you there. Let me know if you have any dietary restrictions or if you have any questions.\\n\\nSincerely,\\n[Your Name]\"}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_rlhf = jload(\"examples/data/outputs_rlhf.json\")[:6]\n",
    "print(\"Example of rlhf output:\\n\")\n",
    "outputs_rlhf[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ff9ed-e67b-41df-b706-f2633ec9b909",
   "metadata": {},
   "source": [
    "The annotator's function of interest when we have two sequences of outputs is `annotate_head2head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e613e0ca-2db8-436c-a4c7-bf71de116ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a72e4e7a-cd40-41c3-87b7-995e323e35d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Dear Friends,\\n\\nI'm planning a dinner party this Friday and I'd love for you to join me! It's going to be a casual night of great food, interesting conversations, and lots of laughs.\\n\\nI'm looking forward to seeing you there. Let me know if you have any dietary restrictions or if you have any questions.\\n\\nSincerely,\\n[Your Name]\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 2.0}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1699e-eef3-4c98-bc83-512f5f401c6f",
   "metadata": {},
   "source": [
    "We see that the format of the output is the same as before. Here `preference` indicates that the simulator prefered the output `output_2`, which corresponds to our RLHF model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6a404-4fc1-4f0c-9858-02f24a189b95",
   "metadata": {},
   "source": [
    "## Training\n",
    "For training we typically have multiple outputs for each instruction which are sampled from the same (SFT) model as seen in the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50354b51-12c5-4833-81af-549d78ef3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of different sampled outputs from SFT:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Why is it important to save money?',\n",
       "  'input': '',\n",
       "  'output': 'Saving money is important because it allows you to have financial security in case of emergencies; to be able to afford the things you need; to have money for retirement; and to be able to afford the lifestyle you want. Saving money can also help you build wealth and reach your financial goals. It is important to develop the habit of regularly putting money aside for savings.'},\n",
       " {'instruction': 'Why is it important to save money?',\n",
       "  'input': '',\n",
       "  'output': 'It is important to save money because it can help create a financial cushion in case of an emergency, allow you to pursue goals, and give you more financial security. Having a savings can also give you the opportunity to make more money by investing it. Saving money can also help you prepare for big expenditures, such as buying a car or a house.'},\n",
       " {'instruction': 'Why is it important to save money?',\n",
       "  'input': '',\n",
       "  'output': 'Saving money is important because it provides financial security and allows for the purchase of large-ticket items, such as a car or house, or for emergency situations. It also helps to create financial independence and allows for more flexibility in life, such as taking unpaid time off from work or retiring early. Having money saved also provides peace of mind, knowing that one is prepared for any unexpected events.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_samples = jload(\"examples/data/multisamples_sft.json\")[:3]\n",
    "print(\"Example of different sampled outputs from SFT:\\n\")\n",
    "outputs_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df755cb-8ff5-478d-b397-682ab3d745e1",
   "metadata": {},
   "source": [
    "In this case, you can use the annotato's `annotate_samples`, which first samples pairs of outputs that have the same instruction/input and then annotate those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc8557da-46fd-4906-a209-a02cc12eef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_samples(outputs_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a55348a-a0ca-4fe1-b6d1-b9fa5d1c339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Why is it important to save money?',\n",
       "  'input': '',\n",
       "  'output_1': 'Saving money is important because it allows you to have financial security in case of emergencies; to be able to afford the things you need; to have money for retirement; and to be able to afford the lifestyle you want. Saving money can also help you build wealth and reach your financial goals. It is important to develop the habit of regularly putting money aside for savings.',\n",
       "  'output_2': 'It is important to save money because it can help create a financial cushion in case of an emergency, allow you to pursue goals, and give you more financial security. Having a savings can also give you the opportunity to make more money by investing it. Saving money can also help you prepare for big expenditures, such as buying a car or a house.',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1.0}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77b705-1845-4994-bd98-17ecfbff3d7f",
   "metadata": {},
   "source": [
    "By default there will only be one pair per instruction,input. If you use `is_unique_instructions=False` then you will get as many pairs as outputs.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515669f8-622c-47f9-9997-c0dedeb797c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec356bc8-82cc-4ab0-aa05-b2f3bbce3408",
   "metadata": {},
   "source": [
    "### Adding noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403b9a8-b444-4861-a837-68fc869dfde9",
   "metadata": {},
   "source": [
    "During training we typically flip the the label with probability 0.25 to emulate the variability of human annotations. To do so you can either:\n",
    "- initialize the annotator with `PairwiseAutoAnnotator(p_label_flip=0.25)`\n",
    "- set the noise of an initialized annotator `annotator.set_noise(p_label_flip=0.25)` \n",
    "- give the noise to annotate_samples `annotate_samples(..., p_label_flip=0.25)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68702d25-3a12-4de6-bef8-e21e79c8d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = annotator.annotate_samples(outputs_samples, p_label_flip=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d663d65-5da5-40b5-a14b-f2f925649132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Why is it important to save money?',\n",
       "  'input': '',\n",
       "  'output_1': 'Saving money is important because it allows you to have financial security in case of emergencies; to be able to afford the things you need; to have money for retirement; and to be able to afford the lifestyle you want. Saving money can also help you build wealth and reach your financial goals. It is important to develop the habit of regularly putting money aside for savings.',\n",
       "  'output_2': 'It is important to save money because it can help create a financial cushion in case of an emergency, allow you to pursue goals, and give you more financial security. Having a savings can also give you the opportunity to make more money by investing it. Saving money can also help you prepare for big expenditures, such as buying a car or a house.',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 2.0}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a596a8-0941-4e19-b19a-ee03dad7c1e1",
   "metadata": {},
   "source": [
    "### Cost & time efficiency: avoiding duplicate annotation\n",
    "Often time you will find yourself reannotating examples that you already annotated. This is particularly true if you sample many times from the same model (eg to get pairwise preferences for training or to evaluate best-of-n) or if the instructions you are considering require short outputs => many models might give the same exact answer.\n",
    "\n",
    "This means that you have to spend unecessary money and time. Thankfully `PairwiseAutoAnnotator` stores previous annotations and will reuse those. For example, let us reannotate the previous evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d43c91-0ef4-417e-bbb2-ce585f6c35ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated = annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5e9051-23fe-4294-9993-0c7afbc74495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Dear Friends,\\n\\nI'm planning a dinner party this Friday and I'd love for you to join me! It's going to be a casual night of great food, interesting conversations, and lots of laughs.\\n\\nI'm looking forward to seeing you there. Let me know if you have any dietary restrictions or if you have any questions.\\n\\nSincerely,\\n[Your Name]\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 2.0}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84836c18-9f26-4b1f-8660-7f2fa39817ee",
   "metadata": {},
   "source": [
    "We now get the annotations without actually having to reannotate any example.\n",
    "By default, this the previous annotations are only stored in memory. If you want those to be stateful, you need to either give `saving_path` to the constructor of `PairwiseAutoAnnotator` (automatically save and loads) or manually call `save` and `load`.\n",
    "Here's an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7c4e90-0648-41ef-b903-71dc0a3f627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = \"examples/data/annotations.json\"\n",
    "annotator.save(saving_path) # manually save the previous annotation\n",
    "new_annotator = PairwiseAutoAnnotator(annotators_config=\"annotators/test/configs.yaml\",\n",
    "                                     saving_path=saving_path # will automatically load and save future annotations\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da900044-8e27-4006-b820-20b3a365c6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Dear Friends,\\n\\nI'm planning a dinner party this Friday and I'd love for you to join me! It's going to be a casual night of great food, interesting conversations, and lots of laughs.\\n\\nI'm looking forward to seeing you there. Let me know if you have any dietary restrictions or if you have any questions.\\n\\nSincerely,\\n[Your Name]\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 2.0}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated = new_annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)\n",
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea60a22-a387-4309-8d1b-b78bdcac5a5a",
   "metadata": {},
   "source": [
    "We suggest that you simply set `saving_path=...` when initializing the annotator and you will never have to deal with manual saving and loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb89a3-7200-4f58-a7f5-9175f36d7cad",
   "metadata": {},
   "source": [
    "### Evaluating win rates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78922f60-a9aa-4383-b18d-c95a93c6546c",
   "metadata": {},
   "source": [
    "Let's show how to get win rates from annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ffaa6c-9570-4384-b157-d6f95b6d90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_baseline = jload(\"examples/data/outputs_baseline.json\")\n",
    "outputs_rlhf = jload(\"examples/data/outputs_rlhf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e000d42-6c37-40c6-81ac-eef3d34a4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_pool = PairwiseAutoAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c05eb80-0f24-4570-86ea-40da1a2a95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 15 examples with gpt4_1\n",
      "INFO:root:Auto annotating 5 samples using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 600, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:56<00:00, 11.31s/it]\n",
      "INFO:root:Annotating 15 examples with gpt4_2\n",
      "INFO:root:Auto annotating 4 samples using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:24<00:00,  6.13s/it]\n",
      "INFO:root:Annotating 22 examples with gpt4_3\n",
      "INFO:root:Auto annotating 6 samples using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:24<00:00,  4.12s/it]\n",
      "WARNING:root:Found 4 preferences in Output (a)\n",
      "Output (a)\n",
      "Output (a)\n",
      "Output (b)\n",
      "Output (a) but expected 5.\n",
      "                    We are setting all preferences to np.nan.\n",
      "WARNING:root:Found 4 preferences in Output (a)\n",
      "Output (a)\n",
      "Output (a)\n",
      "Output (a)\n",
      "Output (a) but expected 5.\n",
      "                    We are setting all preferences to np.nan.\n",
      "WARNING:root:6 samples had no auto annotation. We are filtering them for now. If you are using chain of thought it might be that max_tokens limit is too low. \n",
      "INFO:root:Annotating 23 examples with gpt4_4\n",
      "INFO:root:Auto annotating 23 samples using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:09<00:00,  2.44it/s]\n",
      "INFO:root:Annotating 11 examples with gpt4_5\n",
      "INFO:root:Auto annotating 3 samples using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:28<00:00,  9.53s/it]\n",
      "INFO:root:Annotating 16 examples with chatgpt_1\n",
      "INFO:root:Auto annotating 16 samples using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 50, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:02<00:00,  6.66it/s]\n",
      "INFO:root:Annotating 19 examples with chatgpt_2\n",
      "INFO:root:Auto annotating 19 samples using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:20<00:00,  1.10s/it]\n",
      "INFO:root:Annotating 21 examples with chatgpt_3\n",
      "INFO:root:Auto annotating 21 samples using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches:  71%|████████████████████████████████████████████████████████████████████████████████                                | 15/21 [00:03<00:00,  6.21it/s]WARNING:root:OpenAIError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6679303003bd88f025e8c4b92913ae32 in your message.).\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "WARNING:root:Switching to organization: org-dPb4AamVADaroNRyV4QK3Yyt for OAI API key.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:34<00:00,  1.67s/it]\n",
      "WARNING:root:Found 2 preferences in Output (a) (Output (b) is inappropriate) but expected 1.\n",
      "                    We are setting all preferences to np.nan.\n",
      "WARNING:root:1 samples had no auto annotation. We are filtering them for now. If you are using chain of thought it might be that max_tokens limit is too low. \n",
      "INFO:root:Annotating 25 examples with chatgpt_4\n",
      "INFO:root:Auto annotating 25 samples using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArgumentsChat(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0) {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches:  56%|██████████████████████████████████████████████████████████████▋                                                 | 14/25 [00:16<00:01,  7.64it/s]WARNING:root:OpenAIError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d37999c9576c90f08358c4f9a8d29914 in your message.).\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "WARNING:root:Switching to organization: org-aGS1aYCpHCgg5HGJjdcvj1Gi for OAI API key.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:35<00:00,  1.40s/it]\n",
      "INFO:root:Annotating 21 examples with davinci003_1\n",
      "INFO:root:Auto annotating 6 samples using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: OpenAIDecodingArguments(max_tokens=1800, temperature=0.2, top_p=1.0, n=1, stream=False, stop=None, presence_penalty=0.0, frequency_penalty=0.0, suffix=None, logprobs=None, echo=False) {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.09s/it]\n",
      "WARNING:root:Found 0 preferences in \n",
      "Answer 1 is correct. It provides a link to the ActiveState Recipes site which explains how to read a single character in Windows, Linux, and OSX. Answer 2 is incorrect. The input() function will prompt the user to enter a value, but it will not read a single character from the user input.\n",
      "\n",
      "### Best output for example 8:\n",
      "We need to know, want to know, are eager to know, hope to know, desire to know.\n",
      "\n",
      "### Best output for example 9:\n",
      "I'm sorry to hear that. Can you tell me more about these problems?\n",
      "\n",
      "### Best output for example 10:\n",
      "Crew members can benefit from developing skills such as communication, interpersonal, problem-solving, decision-making, time management, organization, listening, critical thinking, and leadership. Additionally, having knowledge of customer service, safety, and technical skills related to the job (such as first aid or CPR) can help crew members stand out from other but expected 4.\n",
      "                    We are setting all preferences to np.nan.\n",
      "WARNING:root:4 samples had no auto annotation. We are filtering them for now. If you are using chain of thought it might be that max_tokens limit is too low. \n",
      "INFO:root:Annotating 23 examples with davinci003_2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All placeholders should be repeated batch_size=1 times but Counter({'instruction': 5, 'input': 5, 'output_1': 5, 'output_2': 5}).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m to_annotate \u001b[38;5;241m=\u001b[39m annotator_pool\u001b[38;5;241m.\u001b[39mannotate_head2head(outputs_1\u001b[38;5;241m=\u001b[39moutputs_baseline, outputs_2\u001b[38;5;241m=\u001b[39moutputs_rlhf)\n",
      "File \u001b[0;32m~/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/pairwise_annotators.py:239\u001b[0m, in \u001b[0;36mPairwiseAutoAnnotator.annotate_head2head\u001b[0;34m(self, outputs_1, outputs_2, keys_to_merge, is_ordered)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(outputs_1) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(outputs_2) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_to_annotate)):\n\u001b[1;32m    232\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThe length of outputs before and after merge are not the same. We have len(outputs_1)==\u001b[39m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(outputs_1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, len(outputs_2)==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(outputs_2)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, and len(df_annotated)==\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_to_annotate)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124m            This means that there are missing examples or duplicates. We are taking a SQL inner join.\u001b[39m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         )\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotate_pairs(df_to_annotate)\n",
      "File \u001b[0;32m~/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/pairwise_annotators.py:259\u001b[0m, in \u001b[0;36mPairwiseAutoAnnotator.annotate_pairs\u001b[0;34m(self, to_annotate)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    258\u001b[0m df_to_annotate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(to_annotate)\n\u001b[0;32m--> 259\u001b[0m df_annotated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_annotate(df_to_annotate)\n\u001b[1;32m    260\u001b[0m annotated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_and_store_(df_annotated)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m annotated\n",
      "File \u001b[0;32m~/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/pairwise_annotators.py:356\u001b[0m, in \u001b[0;36mPairwiseAutoAnnotator._annotate\u001b[0;34m(self, df_to_annotate)\u001b[0m\n\u001b[1;32m    353\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_idcs\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mannotator\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# actual annotation\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     curr_annotated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotators[annotator](df_annotated[curr_idcs])\n\u001b[1;32m    358\u001b[0m     df_annotated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_annotations(df_annotated, curr_annotated)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_annotated\n",
      "File \u001b[0;32m~/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/pairwise_annotators.py:491\u001b[0m, in \u001b[0;36mSinglePairwiseAutoAnnotator.__call__\u001b[0;34m(self, df_to_annotate)\u001b[0m\n\u001b[1;32m    488\u001b[0m df_to_annotate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(df_to_annotate)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# prompts and completions here will not be the same length as the dataframe due to batching\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m prompts, df_to_annotate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_prompts(df_to_annotate\u001b[38;5;241m=\u001b[39mdf_to_annotate)\n\u001b[1;32m    493\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn_decoder(prompts\u001b[38;5;241m=\u001b[39mprompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_kwargs)\n\u001b[1;32m    495\u001b[0m df_to_annotate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreference\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_completions(completions\u001b[38;5;241m=\u001b[39mcompletions)\n",
      "File \u001b[0;32m~/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/pairwise_annotators.py:546\u001b[0m, in \u001b[0;36mSinglePairwiseAutoAnnotator.make_prompts\u001b[0;34m(self, df_to_annotate)\u001b[0m\n\u001b[1;32m    542\u001b[0m prompts, df \u001b[38;5;241m=\u001b[39m ann_utils\u001b[38;5;241m.\u001b[39mmake_prompts(\n\u001b[1;32m    543\u001b[0m     df_without_inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_templates[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    544\u001b[0m )\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr_is_inputs\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 546\u001b[0m     prompts_i, df_i \u001b[38;5;241m=\u001b[39m ann_utils\u001b[38;5;241m.\u001b[39mmake_prompts(\n\u001b[1;32m    547\u001b[0m         df_with_inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_templates[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    548\u001b[0m     )\n\u001b[1;32m    549\u001b[0m     prompts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prompts_i\n\u001b[1;32m    550\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, df_i], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/utils.py:184\u001b[0m, in \u001b[0;36mmake_prompts\u001b[0;34m(df, template, batch_size, padding_example)\u001b[0m\n\u001b[1;32m    181\u001b[0m n_occurrences \u001b[38;5;241m=\u001b[39m Counter(text_to_format)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([n \u001b[38;5;241m==\u001b[39m batch_size \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_occurrences\u001b[38;5;241m.\u001b[39mvalues()]):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll placeholders should be repeated batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_occurrences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# padding if you don't have enough examples\u001b[39;00m\n\u001b[1;32m    187\u001b[0m n_to_pad \u001b[38;5;241m=\u001b[39m (batch_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)) \u001b[38;5;241m%\u001b[39m batch_size\n",
      "\u001b[0;31mValueError\u001b[0m: All placeholders should be repeated batch_size=1 times but Counter({'instruction': 5, 'input': 5, 'output_1': 5, 'output_2': 5})."
     ]
    }
   ],
   "source": [
    "to_annotate = annotator_pool.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febabf6-4525-49a8-9c45-c0a825326338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e703c1ea-8406-4599-8a43-d5a5de63459e",
   "metadata": {},
   "source": [
    "### Configuring annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927636c-96f5-4125-a553-a5be011c2a42",
   "metadata": {},
   "source": [
    "The most important argument to `PairwiseAutoAnnotator` is `annotators_config` which defines the pool of annotators. We provide the following options out of the box:\n",
    "- `annotators_config=\"annotators/annotator_pool_v0/configs.yaml\"`: ApacaFarm's default annotator pool\n",
    "- `annotators_config=\"annotators/greedy_gpt4/configs.yaml\"`: Greedy GPT4 annotator\n",
    "- `annotators_config=\"annotators/test/configs.yaml\"`: a faster text-davinci-003 annotator useful for testing\n",
    "\n",
    "Here's the desciption of `annotators_config` from the docstring:\n",
    "```\n",
    "annotators_config : Path or list of dict\n",
    "    A dictionary or path to a yaml file containing the configuration for the pool of annotators. The keys in the\n",
    "        fist dictionary should be the annotator's name, and the value should be a dictionary of the annotator's\n",
    "        configuration which should have the following keys:\n",
    "        - prompt_templates (dict): a dictionary of prompts or path to the prompts.\n",
    "        - fn_decoder (str): function in `alpaca_farm.auto_annotations.pairwise_annotators.decoders.py` for completions.\n",
    "        - decoder_kwargs (dict): kwargs for fn_decode. E.g. model_name, max_completions_tokens\n",
    "        - other kwargs to `SinglePairwiseAutoAnnotator`\n",
    "```\n",
    "\n",
    "And here is config `\"annotators/test/configs.yaml\"` of the annotator we used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "945ba06f-cc13-436f-83f0-f074f37ea2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci003_3 : # text-davinci-003_v1_b5-pairwise_temp=1.0\n",
      "  prompt_templates:\n",
      "    with_inputs: \"annotators/annotator_pool_v0/text_b5_with_inputs.txt\"\n",
      "    without_inputs: \"annotators/annotator_pool_v0/text_b5_without_inputs.txt\"\n",
      "  fn_decoder: \"openai_completions\"\n",
      "  decoder_kwargs:\n",
      "    model_name: \"text-davinci-003\"\n",
      "    max_tokens: 200\n",
      "    temperature: 0.0\n",
      "  outputs_to_match:\n",
      "    1: '\\n\\(a\\)'\n",
      "    2: '\\n\\(b\\)'\n",
      "  batch_size: 5"
     ]
    }
   ],
   "source": [
    "!cat src/alpaca_farm/auto_annotations/annotators/test/configs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233e875-ae40-4b53-bc6b-05cf0201bbf0",
   "metadata": {},
   "source": [
    "For more information see the docstrings of the following classes (eg uncomment and run the following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "459aa856-241b-451b-8917-da24b6f85052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from alpaca_farm.auto_annotations.pairwise_annotators import SinglePairwiseAutoAnnotator\n",
    "# PairwiseAutoAnnotator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "730ed569-d910-434c-a5d9-6600cab17701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SinglePairwiseAutoAnnotator?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d886bf-3002-4939-8751-332cf4ef0e35",
   "metadata": {},
   "source": [
    "If you want to make a new annotator, change the prompt and or the configs above. See `annotators/annotator_pool_v0/...txt` for the actual prompts we used. To see the prompt used above (which is part of our pool of prompts) uncomment and run the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bb96d79-e4e6-407c-b7e0-758ec75bc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cat src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/text_b5_without_inputs.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
