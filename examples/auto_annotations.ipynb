{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839ee94d-5ead-46da-bd60-5b11e4aaebc4",
   "metadata": {},
   "source": [
    "# Auto Annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ee20f0-007c-4f6a-af1c-ad39b959984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yanndubois/Desktop/GitHub/alpaca_farm\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bf8b0-90b2-4691-b6d8-cef65cbde034",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f2875-9f29-4662-b63e-f96a8887aa14",
   "metadata": {},
   "source": [
    "All of our annotators currently use OpenAI API. So the first step is to setup your OpenAI key (and potentially your organization). This can be done by either running setting your environment variable like \n",
    "- `export OPENAI_API_KEY=\"sk...\"` \n",
    "\n",
    "or by setting the following python variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160c6cd6-dbb5-49ab-a286-344ce2e87308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    decoding_kwargs = dict(\n",
    "        openai_api_key = None, #\"sk-...\",\n",
    "        openai_organization_ids = None, # [\"org-...\",\"org-...\"] you can set multiple orgs to avoid rate limits\n",
    "    )\n",
    "    assert decoding_kwargs[\"openai_api_key\"] is not None, \"OPENAI_API_KEY not found you should set it in environment or above\"\n",
    "else:\n",
    "    decoding_kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8222f-c017-4aae-9b29-cc8210e946fe",
   "metadata": {},
   "source": [
    "The key object that you need for automatic annotations (both for training or for eval) is our `PairwiseAutoAnnotator` which inherits from [AlapcaEval's `PairwiseAnnotator`](https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/annotators/pairwise_evaluator.py#L20). By default the annotator is the pool from Alpaca Farm, for simplicity let's use a single annotator `test` for now.\n",
    "\n",
    "For an overview annotators including how to extend them (different model such as ChatGPT or GPT4, prompts, decoding parameters), refer to the [Configuring annotators](#Configuring-annotators) section of this notebook. For more details refer to [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d6dcf4-b0fb-4c56-a1fd-1db509af05e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating the annotator from `test`.\n",
      "INFO:root:Saving annotations to `/Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "from alpaca_farm.utils import jload\n",
    "from alpaca_farm.auto_annotations import PairwiseAutoAnnotator\n",
    "\n",
    "annotator = PairwiseAutoAnnotator(annotators_config=\"test\", **decoding_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99ac04-0d48-410e-95fd-cf3b1360fc4e",
   "metadata": {},
   "source": [
    "## Annotating paired outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef99a2c-77ef-41e6-9db8-eae091afd952",
   "metadata": {},
   "source": [
    "Now let's annotate some pairwise preference. All we need is some data. The annotator takes in either list of dictionaries, or pandas dataframes. The keys of dictionaries (or columns in dataframe) need to always contain `instruction` and `input` which defines the instruction. The annotator also needs a pair of outputs to compare, if you have a sequence of such pairs under the keys `output_1` and `output_2` then you can directly use `annotate_pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab42c1d-3bda-4797-a1bd-877b958e8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of paired output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Hey everyone! \\n\\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \\n\\nLet me know if you can make it - I'd love to see you all there!\\n\\nCheers,\\n[Your Name]\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_pairs = jload(\"examples/data/outputs_pairs.json\")[:6]\n",
    "print(\"Example of paired output:\\n\")\n",
    "outputs_pairs[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67caa934-5a37-429a-affc-dbd182be7c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 0 examples with davinci003_3\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_pairs(outputs_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd953df-7eb1-4446-86a9-f3b3fbfba665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Hey everyone! \\n\\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \\n\\nLet me know if you can make it - I'd love to see you all there!\\n\\nCheers,\\n[Your Name]\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648a4b0-5d19-4bc7-b63e-d4cb4ab4fd51",
   "metadata": {},
   "source": [
    "Here we see that the annotations adds two keys:\n",
    "- `'preference'`: the index of the preferred output, here `preference=2` so `output_1` is prefered. In the case where both outputs are the same we give `preference=0`\n",
    "- `'annotator'`: the name of the simulated annotator as found in the `annotators_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ee339-cf40-4a3c-a861-19c265f962bc",
   "metadata": {},
   "source": [
    "`annotate_pairs` is the main function and should be used if you have paired outputs to annotate. In many usecases, however, you  will outputs in different formats. In the following we discuss two additional helper function `annotate_head2head` and `annotate_samples` which are paticularly well suited for the typical format during evaluation and training respectively. Both functions call `annotate_pairs` under the hood after a reformatting step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b41af4-731c-4d12-83b4-9b69f4b2bc3f",
   "metadata": {},
   "source": [
    "## Evaluation through pairwise comparison\n",
    "For evaluation we need two components:\n",
    "- outputs from the model we want to compare\n",
    "- outputs on the same examples from the baseline model\n",
    "\n",
    "Often case both of those components will be in a different list of dictionary (one list for each model). In this case all dictionaries need to contain an `output` column. Let us load such data from our simulated RLHF model and text-davinci-003 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a9cce3-0fe9-4dc2-9b49-de59f641e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of baseline output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_baseline = jload(\"examples/data/outputs_baseline.json\")[:6]\n",
    "print(\"Example of baseline output:\\n\")\n",
    "outputs_baseline[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f808315-da5c-4af3-a026-64e3d07801f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of rlhf output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output': 'Dear Friends, \\n\\nI am writing to invite you all to a dinner on Friday evening. It is a casual affair, and I am looking forward to a fun evening catching up with you all. I am planning to make a selection of delicious dishes, ranging from appetizers to mains and desserts. There will be something for everyone to enjoy, and I am sure it will be a night to remember.\\n\\nThe dinner will be held at my place on Friday, April 17th at 7pm. If you are interested in joining me, please RSVP to this email by Thursday, April 16th. I am looking forward to seeing you all there! \\n\\nThank you, \\n\\n[Name]'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_rlhf = jload(\"examples/data/outputs_rlhf.json\")[:6]\n",
    "print(\"Example of rlhf output:\\n\")\n",
    "outputs_rlhf[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ff9ed-e67b-41df-b706-f2633ec9b909",
   "metadata": {},
   "source": [
    "The annotator's function of interest when we have two sequences of outputs is `annotate_head2head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e613e0ca-2db8-436c-a4c7-bf71de116ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 0 examples with davinci003_3\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72e4e7a-cd40-41c3-87b7-995e323e35d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'The sentence you are given might be too wordy, complicated, or unclear. Rewrite the sentence and make your writing clearer by keeping it concise. Whenever possible, break complex sentences into multiple sentences and eliminate unnecessary words.',\n",
       "  'input': 'If you have any questions about my rate or if you find it necessary to increase or decrease the scope for this project, please let me know.',\n",
       "  'output_1': 'If you have questions about my rate or need to modify the scope of this project, please let me know.',\n",
       "  'output_2': 'I am available to answer any questions you may have about my rate or if you need to alter the scope of this project. Feel free to contact me if you have any queries or require any additional information.',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1699e-eef3-4c98-bc83-512f5f401c6f",
   "metadata": {},
   "source": [
    "We see that the format of the output is the same as before. Here `preference` indicates that the simulator prefered the output `output_1`, which corresponds to the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6a404-4fc1-4f0c-9858-02f24a189b95",
   "metadata": {},
   "source": [
    "## Training\n",
    "For training we typically have multiple outputs for each instruction which are sampled from the same (SFT) model as seen in the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50354b51-12c5-4833-81af-549d78ef3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of different sampled outputs from SFT:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Describe a time when you had to make a difficult decision.',\n",
       "  'input': '',\n",
       "  'output': [\"I had to make a difficult decision a few years ago when I was offered a job that would require me to move to a different city. I had to weigh the pros and cons of this job offer, and consider what it would mean for me to leave my friends and family in order to take it. In the end, I decided to turn down the job offer since I wasn't ready to quit my current job, move to a different city, and leave my loved ones behind.\",\n",
       "   'I had to make a difficult decision last year when I was faced with a life-changing opportunity. I had to decide whether or not to leave my current job and move to a different city for the chance to further my career. After much consideration, I chose to take the risk and make the move. Thankfully, it paid off and I am now enjoying my new job and exploring the city.',\n",
       "   \"I had to make a difficult decision when I was offered a job in a city that was significantly farther away from my friends and family. I weighed the pros and cons and consulted with people I trusted, but in the end I had to make the choice that was best for my future and my well-being. It was a difficult decision, but I'm glad I made the choice that allowed me to further my career and start my journey towards independence.\",\n",
       "   'I remember a time when I had to decide whether to stay in my hometown or move to a new city. It was a difficult decision because I had grown up in the town, and I was used to the familiarity of my childhood home. On the other hand, the new city had more job opportunities and a better environment that I thought would be beneficial for my future. After weighing the pros and cons, I made the decision to move to the new city. It was the right choice, as I was able to find a job I enjoyed and make some great friends in the new place.']}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_samples = jload(\"examples/data/multisamples_sft.json\")[:1]\n",
    "print(\"Example of different sampled outputs from SFT:\\n\")\n",
    "outputs_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df755cb-8ff5-478d-b397-682ab3d745e1",
   "metadata": {},
   "source": [
    "In this case, you can use the annotator's `annotate_samples`, which first samples pairs of outputs that have the same instruction/input and then annotate those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc8557da-46fd-4906-a209-a02cc12eef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Filtered unique instruction/input pairs: 4 -> 1\n",
      "INFO:root:Annotating 0 examples with davinci003_3\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_samples(outputs_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a55348a-a0ca-4fe1-b6d1-b9fa5d1c339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_id': 0,\n",
       "  'instruction': 'Describe a time when you had to make a difficult decision.',\n",
       "  'input': '',\n",
       "  'output_1': \"I had to make a difficult decision a few years ago when I was offered a job that would require me to move to a different city. I had to weigh the pros and cons of this job offer, and consider what it would mean for me to leave my friends and family in order to take it. In the end, I decided to turn down the job offer since I wasn't ready to quit my current job, move to a different city, and leave my loved ones behind.\",\n",
       "  'output_2': \"I had to make a difficult decision when I was offered a job in a city that was significantly farther away from my friends and family. I weighed the pros and cons and consulted with people I trusted, but in the end I had to make the choice that was best for my future and my well-being. It was a difficult decision, but I'm glad I made the choice that allowed me to further my career and start my journey towards independence.\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77b705-1845-4994-bd98-17ecfbff3d7f",
   "metadata": {},
   "source": [
    "By default there will only be one pair per instruction,input. If you use `is_unique_instructions=False` then you will get as many pairs as outputs.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515669f8-622c-47f9-9997-c0dedeb797c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec356bc8-82cc-4ab0-aa05-b2f3bbce3408",
   "metadata": {},
   "source": [
    "### Adding noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403b9a8-b444-4861-a837-68fc869dfde9",
   "metadata": {},
   "source": [
    "During training we typically flip the the label with probability 0.25 to emulate the variability of human annotations. To do so you can either:\n",
    "- initialize the annotator with `PairwiseAutoAnnotator(p_label_flip=0.25)`\n",
    "- set the noise of an initialized annotator `annotator.set_noise(p_label_flip=0.25)` \n",
    "- give the noise to annotate_samples `annotate_samples(..., p_label_flip=0.25)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68702d25-3a12-4de6-bef8-e21e79c8d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Filtered unique instruction/input pairs: 4 -> 1\n",
      "INFO:root:Adding random noise to the labels p_label_flip=0.25.\n",
      "INFO:root:Annotating 0 examples with davinci003_3\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_samples(outputs_samples, p_label_flip=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d663d65-5da5-40b5-a14b-f2f925649132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_id': 0,\n",
       "  'instruction': 'Describe a time when you had to make a difficult decision.',\n",
       "  'input': '',\n",
       "  'output_1': \"I had to make a difficult decision a few years ago when I was offered a job that would require me to move to a different city. I had to weigh the pros and cons of this job offer, and consider what it would mean for me to leave my friends and family in order to take it. In the end, I decided to turn down the job offer since I wasn't ready to quit my current job, move to a different city, and leave my loved ones behind.\",\n",
       "  'output_2': \"I had to make a difficult decision when I was offered a job in a city that was significantly farther away from my friends and family. I weighed the pros and cons and consulted with people I trusted, but in the end I had to make the choice that was best for my future and my well-being. It was a difficult decision, but I'm glad I made the choice that allowed me to further my career and start my journey towards independence.\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a596a8-0941-4e19-b19a-ee03dad7c1e1",
   "metadata": {},
   "source": [
    "### Cost & time efficiency: avoiding duplicate annotation\n",
    "Often time you will find yourself reannotating examples that you already annotated. This is particularly true if you sample many times from the same model (eg to get pairwise preferences for training or to evaluate best-of-n) or if the instructions you are considering require short outputs => many models might give the same exact answer.\n",
    "\n",
    "This means that you have to spend unecessary money and time. Thankfully `PairwiseAutoAnnotator` stores previous annotations and will reuse those. For example, let us reannotate the previous evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d43c91-0ef4-417e-bbb2-ce585f6c35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 0 examples with davinci003_3\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/test/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5e9051-23fe-4294-9993-0c7afbc74495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': 'Dear Friends, \\n\\nI am writing to invite you all to a dinner on Friday evening. It is a casual affair, and I am looking forward to a fun evening catching up with you all. I am planning to make a selection of delicious dishes, ranging from appetizers to mains and desserts. There will be something for everyone to enjoy, and I am sure it will be a night to remember.\\n\\nThe dinner will be held at my place on Friday, April 17th at 7pm. If you are interested in joining me, please RSVP to this email by Thursday, April 16th. I am looking forward to seeing you all there! \\n\\nThank you, \\n\\n[Name]',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 2}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84836c18-9f26-4b1f-8660-7f2fa39817ee",
   "metadata": {},
   "source": [
    "We now get the annotations without actually having to reannotate any example.\n",
    "By default, the annotations are saved on disk in the directory that contains the annotators configs. You can remove this caching by giving `caching_path=None` to `PairwiseAutoAnnotator`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb89a3-7200-4f58-a7f5-9175f36d7cad",
   "metadata": {},
   "source": [
    "### Evaluating win rates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78922f60-a9aa-4383-b18d-c95a93c6546c",
   "metadata": {},
   "source": [
    "Let's show how to get win rates from annotations using our actual pool of annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "826421f1-e644-42c9-9375-d8b08fb958ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_eval.metrics import pairwise_to_winrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ffaa6c-9570-4384-b157-d6f95b6d90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the following is selfinstruct eval\n",
    "outputs_baseline = jload(\"examples/data/outputs_baseline.json\")\n",
    "outputs_rlhf = jload(\"examples/data/outputs_rlhf.json\")\n",
    "outputs_sft = jload(\"examples/data/outputs_sft.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e000d42-6c37-40c6-81ac-eef3d34a4816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating the annotator from `annotator_pool_v0`.\n",
      "INFO:root:Saving annotations to `/Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json`.\n"
     ]
    }
   ],
   "source": [
    "annotator_pool = PairwiseAutoAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74b79ba1-aeb3-4dd0-8669-a171ba4a2c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 18 examples with gpt4_1\n",
      "INFO:root:Using `openai_completions` on 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 600, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 600, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:04<00:00, 12.94s/it]\n",
      "INFO:root:Completed 5 examples in 64.8 seconds.\n",
      "INFO:root:Annotating 20 examples with gpt4_2\n",
      "INFO:root:Using `openai_completions` on 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 250, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.89s/it]\n",
      "INFO:root:Completed 5 examples in 19.6 seconds.\n",
      "INFO:root:Annotating 19 examples with gpt4_3\n",
      "INFO:root:Using `openai_completions` on 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 250, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.54s/it]\n",
      "INFO:root:Completed 5 examples in 17.8 seconds.\n",
      "INFO:root:Annotating 19 examples with gpt4_4\n",
      "INFO:root:Using `openai_completions` on 19 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:08<00:00,  2.17it/s]\n",
      "INFO:root:Completed 19 examples in 8.9 seconds.\n",
      "INFO:root:Annotating 14 examples with gpt4_5\n",
      "INFO:root:Using `openai_completions` on 3 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 250, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:18<00:00,  6.20s/it]\n",
      "INFO:root:Completed 3 examples in 18.7 seconds.\n",
      "INFO:root:Annotating 23 examples with chatgpt_1\n",
      "INFO:root:Using `openai_completions` on 23 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 50, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 50, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:05<00:00,  4.08it/s]\n",
      "INFO:root:Completed 23 examples in 5.7 seconds.\n",
      "INFO:root:Annotating 31 examples with chatgpt_2\n",
      "INFO:root:Using `openai_completions` on 31 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:29<00:00,  1.06it/s]\n",
      "INFO:root:Completed 31 examples in 29.5 seconds.\n",
      "INFO:root:Annotating 14 examples with chatgpt_3\n",
      "INFO:root:Using `openai_completions` on 14 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:04<00:00,  3.34it/s]\n",
      "INFO:root:Completed 14 examples in 4.3 seconds.\n",
      "INFO:root:Annotating 19 examples with chatgpt_4\n",
      "INFO:root:Using `openai_completions` on 19 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:04<00:00,  3.95it/s]\n",
      "INFO:root:Completed 19 examples in 4.9 seconds.\n",
      "INFO:root:Annotating 10 examples with davinci003_1\n",
      "INFO:root:Using `openai_completions` on 3 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0, 'logit_bias': {7: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0, 'logit_bias': {7: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.86s/it]\n",
      "INFO:root:Completed 3 examples in 3.9 seconds.\n",
      "INFO:root:Annotating 17 examples with davinci003_2\n",
      "INFO:root:Using `openai_completions` on 17 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.36s/it]\n",
      "INFO:root:Completed 17 examples in 4.7 seconds.\n",
      "INFO:root:Annotating 14 examples with davinci003_3\n",
      "INFO:root:Using `openai_completions` on 3 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.94s/it]\n",
      "INFO:root:Completed 3 examples in 4.0 seconds.\n",
      "INFO:root:Annotating 24 examples with davinci003_4\n",
      "INFO:root:Using `openai_completions` on 24 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.00it/s]\n",
      "INFO:root:Completed 24 examples in 3.0 seconds.\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'win_rate': 30.555555555555557,\n",
       " 'standard_error': 2.838766377491875,\n",
       " 'n_wins': 72,\n",
       " 'n_wins_base': 170,\n",
       " 'n_draws': 10,\n",
       " 'n_total': 252}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_sft = annotator_pool.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_sft)\n",
    "pairwise_to_winrate(preferences=[a[\"preference\"] for a in annotated_sft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c05eb80-0f24-4570-86ea-40da1a2a95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 19 examples with gpt4_1\n",
      "INFO:root:Using `openai_completions` on 6 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 600, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 600, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:21<00:00, 13.50s/it]\n",
      "INFO:root:Completed 6 examples in 81.1 seconds.\n",
      "INFO:root:Annotating 19 examples with gpt4_2\n",
      "INFO:root:Using `openai_completions` on 4 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 250, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:26<00:00,  6.53s/it]\n",
      "INFO:root:Completed 4 examples in 26.2 seconds.\n",
      "INFO:root:Annotating 20 examples with gpt4_3\n",
      "INFO:root:Using `openai_completions` on 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 250, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:21<00:00,  4.33s/it]\n",
      "INFO:root:Completed 5 examples in 21.7 seconds.\n",
      "INFO:root:Annotating 19 examples with gpt4_4\n",
      "INFO:root:Using `openai_completions` on 19 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:06<00:00,  2.92it/s]\n",
      "INFO:root:Completed 19 examples in 6.6 seconds.\n",
      "INFO:root:Annotating 14 examples with gpt4_5\n",
      "INFO:root:Using `openai_completions` on 3 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 250, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:21<00:00,  7.08s/it]\n",
      "INFO:root:Completed 3 examples in 21.3 seconds.\n",
      "INFO:root:Annotating 24 examples with chatgpt_1\n",
      "INFO:root:Using `openai_completions` on 24 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 50, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 50, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:05<00:00,  4.35it/s]\n",
      "INFO:root:Completed 24 examples in 5.6 seconds.\n",
      "INFO:root:Annotating 31 examples with chatgpt_2\n",
      "INFO:root:Using `openai_completions` on 31 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:31<00:00,  1.02s/it]\n",
      "INFO:root:Completed 31 examples in 31.8 seconds.\n",
      "INFO:root:Annotating 14 examples with chatgpt_3\n",
      "INFO:root:Using `openai_completions` on 14 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:04<00:00,  3.47it/s]\n",
      "INFO:root:Completed 14 examples in 4.1 seconds.\n",
      "INFO:root:Annotating 20 examples with chatgpt_4\n",
      "INFO:root:Using `openai_completions` on 20 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.44it/s]\n",
      "INFO:root:Completed 20 examples in 4.6 seconds.\n",
      "INFO:root:Annotating 10 examples with davinci003_1\n",
      "INFO:root:Using `openai_completions` on 3 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0, 'logit_bias': {7: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0, 'logit_bias': {7: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/it]\n",
      "INFO:root:Completed 3 examples in 3.1 seconds.\n",
      "INFO:root:Annotating 18 examples with davinci003_2\n",
      "INFO:root:Using `openai_completions` on 18 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  2.00s/it]\n",
      "INFO:root:Completed 18 examples in 4.0 seconds.\n",
      "INFO:root:Annotating 15 examples with davinci003_3\n",
      "INFO:root:Using `openai_completions` on 3 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.22s/it]\n",
      "INFO:root:Completed 3 examples in 6.2 seconds.\n",
      "INFO:root:Annotating 26 examples with davinci003_4\n",
      "INFO:root:Using `openai_completions` on 26 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:03<00:00,  1.16s/it]\n",
      "INFO:root:Completed 26 examples in 3.5 seconds.\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'win_rate': 47.42063492063492,\n",
       " 'standard_error': 3.1454933555275866,\n",
       " 'n_wins': 119,\n",
       " 'n_wins_base': 132,\n",
       " 'n_draws': 1,\n",
       " 'n_total': 252}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_rlhf = annotator_pool.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)\n",
    "pairwise_to_winrate(preferences=[a[\"preference\"] for a in annotated_rlhf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ea9d199-08c4-4b94-9628-ef28a2c9e733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bdcc8327ff849ed8f8bdcd9142bf0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be8be328d6a4a808480900ed3bfcc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset alpaca_farm/alpaca_farm_evaluation to /Users/yanndubois/.cache/huggingface/datasets/tatsu-lab___alpaca_farm/alpaca_farm_evaluation/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f543c1da09a4a799253caea4d4b86e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/600k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yanndubois/.cache/huggingface/datasets/downloads/b8c17ea4d3dc2405e3c93ddaea295a6fe540c820557b668c6ac40cb66e1f606d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset alpaca_farm downloaded and prepared to /Users/yanndubois/.cache/huggingface/datasets/tatsu-lab___alpaca_farm/alpaca_farm_evaluation/1.0.0/79d38dc3f12abd62869e376303b68092e8385769e22f05166fe96a3dac29a57a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498681bfc65044f5bb553857e9f6a85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from alpaca_farm import constants\n",
    "outputs_baseline = datasets.load_dataset(\"tatsu-lab/alpaca_farm\",\n",
    "                                             \"alpaca_farm_evaluation\",\n",
    "                                             cache_dir=constants.DEFAULT_CACHE_DIR,\n",
    "                                             )['eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd2138be-2cbe-432e-8bc5-54cb1a7439e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating the annotator from `annotator_pool_v0`.\n",
      "INFO:root:Saving annotations to `/Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json`.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "annotator = PairwiseAutoAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba53eb9-d5a3-496d-8ae4-969b54ba2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_2 = outputs_baseline.to_pandas()[:10]\n",
    "outputs_2.output = outputs_2.output.str[:20]\n",
    "outputs_1  = outputs_baseline.to_pandas()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bcc720a-3ac4-4228-86c1-a5fbace62c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The length of outputs before and after merge are not the same. We have len(outputs_1)==\n",
      "                    805, len(outputs_2)==10, and len(df_annotated)==10. \n",
      "                    This means that there are missing examples or duplicates. We are taking a SQL inner join.\n",
      "                    \n",
      "INFO:root:Annotating 0 examples with gpt4_1\n",
      "INFO:root:Annotating 2 examples with gpt4_2\n",
      "INFO:root:Using `openai_completions` on 1 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-4-0314', 'is_chat': True, 'max_tokens': 250, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.74s/it]\n",
      "INFO:root:Completed 1 examples in 17.8 seconds.\n",
      "INFO:root:Annotating 0 examples with gpt4_3\n",
      "INFO:root:Annotating 0 examples with gpt4_4\n",
      "INFO:root:Annotating 0 examples with gpt4_5\n",
      "INFO:root:Annotating 0 examples with chatgpt_1\n",
      "INFO:root:Annotating 2 examples with chatgpt_2\n",
      "INFO:root:Using `openai_completions` on 2 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.11s/it]\n",
      "INFO:root:Completed 2 examples in 4.3 seconds.\n",
      "INFO:root:Annotating 1 examples with chatgpt_3\n",
      "INFO:root:Using `openai_completions` on 1 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "INFO:root:Completed 1 examples in 1.8 seconds.\n",
      "INFO:root:Annotating 1 examples with chatgpt_4\n",
      "INFO:root:Using `openai_completions` on 1 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'gpt-3.5-turbo-0301', 'is_chat': True, 'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "INFO:root:Completed 1 examples in 1.6 seconds.\n",
      "INFO:root:Annotating 1 examples with davinci003_1\n",
      "INFO:root:Using `openai_completions` on 1 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0, 'logit_bias': {7: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0, 'logit_bias': {7: 7, 64: 7, 8: 7, 65: 7}}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.40s/it]\n",
      "INFO:root:Completed 1 examples in 3.4 seconds.\n",
      "INFO:root:Annotating 1 examples with davinci003_2\n",
      "INFO:root:Using `openai_completions` on 1 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "INFO:root:Completed 1 examples in 1.2 seconds.\n",
      "INFO:root:Annotating 2 examples with davinci003_3\n",
      "INFO:root:Using `openai_completions` on 1 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Kwargs to completion: {'n': 1, 'model': 'text-davinci-003', 'is_chat': False, 'max_tokens': 200, 'temperature': 1.0}\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.50s/it]\n",
      "INFO:root:Completed 1 examples in 4.5 seconds.\n",
      "INFO:root:Annotating 0 examples with davinci003_4\n",
      "INFO:root:Saving all annotations to /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json.\n",
      "INFO:root:Loading all annotations from /Users/yanndubois/Desktop/GitHub/alpaca_farm/src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/annotations_seed0_configs.json.\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703c1ea-8406-4599-8a43-d5a5de63459e",
   "metadata": {},
   "source": [
    "### Configuring annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927636c-96f5-4125-a553-a5be011c2a42",
   "metadata": {},
   "source": [
    "The most important argument to `PairwiseAutoAnnotator` is `annotators_config` which defines the pool of annotators (the API provider, the model, the prompt, and the decoding parameters) . We provide the following options out of the box:\n",
    "- `annotators_config=\"annotators/annotator_pool_v0/configs.yaml\"`: ApacaFarm's default annotator pool\n",
    "- `annotators_config=\"annotators/greedy_gpt4/configs.yaml\"`: Greedy GPT4 annotator\n",
    "- `annotators_config=\"annotators/test/configs.yaml\"`: a faster text-davinci-003 annotator useful for testing\n",
    "\n",
    "Here's the desciption of `annotators_config` from the docstring:\n",
    "```\n",
    "A dictionary or path to a yaml file containing the configuration for the pool of annotators. The keys in the first dictionary should be the annotator's name, and the value should be a dictionary of the annotator's configuration which should have the following keys:\n",
    "\n",
    "- prompt_templates (dict): a dictionary of prompt templates or path to the prompts. The keys should be \"without_inputs\" and \"with_inputs\". Each template should contain placeholders for keys in the example dictionary, typically {instruction} and {output_1} {output_2}.\n",
    "- fn_decoder (str): function in `alpaca_farm.auto_annotations.pairwise_annotators.decoders.py` for completions.\n",
    "- decoder_kwargs (dict): kwargs for fn_decode. E.g. model_name, max_tokens, temperature, tokens_to_avoid, tokens_to_favor\n",
    "- outputs_to_match (dict): Kwargs for fn_completion_parser. With the default regex parser it needs `outputs_to_match` which is a dictionary of outputs to match from the completions. The values should be a regex pattern that should be matched, the keys should be the corresponding preference value. For example {1: 'Output \\(a\\)'} will match the output \"Output (a)\" and set the preference to 1.\n",
    "- other kwargs to `SinglePairwiseAutoAnnotator` such as batch_size\n",
    "```\n",
    "\n",
    "And here is config `\"annotators/test/configs.yaml\"` of the annotator we used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "945ba06f-cc13-436f-83f0-f074f37ea2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci003_3 : # text-davinci-003_v1_b5-pairwise_temp=1.0\n",
      "  prompt_template:\n",
      "    with_inputs: \"annotator_pool_v0/text_b5_with_inputs.txt\"\n",
      "    without_inputs: \"annotator_pool_v0/text_b5_without_inputs.txt\"\n",
      "  fn_completions: \"openai_completions\"\n",
      "  completions_kwargs:\n",
      "    model_name: \"text-davinci-003\"\n",
      "    max_tokens: 200\n",
      "    temperature: 1.0\n",
      "  completion_parser_kwargs:\n",
      "    outputs_to_match:\n",
      "      1: '\\n\\(a\\)'\n",
      "      2: '\\n\\(b\\)'\n",
      "  batch_size: 5\n"
     ]
    }
   ],
   "source": [
    "!cat src/alpaca_farm/auto_annotations/annotators/test/configs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233e875-ae40-4b53-bc6b-05cf0201bbf0",
   "metadata": {},
   "source": [
    "For more information, please refer to [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval/tree/main). In our code, we directly use AlpacaEval, with a minor modification to separate instructions with and without inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27789c-e0db-49c8-83ad-efae843dbea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
