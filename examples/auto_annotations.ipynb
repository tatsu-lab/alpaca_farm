{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839ee94d-5ead-46da-bd60-5b11e4aaebc4",
   "metadata": {},
   "source": [
    "# Auto Annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9ee20f0-007c-4f6a-af1c-ad39b959984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yanndubois/Desktop/GitHub/alpaca_farm\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bf8b0-90b2-4691-b6d8-cef65cbde034",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f2875-9f29-4662-b63e-f96a8887aa14",
   "metadata": {},
   "source": [
    "All of our annotators currently use OpenAI API. So the first step is to setup your OpenAI key (and potentially your organization). This can be done by either running setting your environment variable like \n",
    "- `export OPENAI_API_KEY=\"sk...\"` \n",
    "\n",
    "or by setting the following python variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160c6cd6-dbb5-49ab-a286-344ce2e87308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    decoding_kwargs = dict(\n",
    "        openai_api_key = None, #\"sk-...\",\n",
    "        openai_organization_ids = None, # [\"org-...\",\"org-...\"] you can set multiple orgs to avoid rate limits\n",
    "    )\n",
    "    assert decoding_kwargs[\"openai_api_key\"] is not None, \"OPENAI_API_KEY not found you should set it in environment or above\"\n",
    "else:\n",
    "    decoding_kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b8222f-c017-4aae-9b29-cc8210e946fe",
   "metadata": {},
   "source": [
    "The key object that you need for automatic annotations (both for training or for eval) is our `PairwiseAutoAnnotator`. By default the annotator is the pool from Alpaca Farm, for simplicity let's use a single annotator `test` for now.\n",
    "\n",
    "For details about annotators including how to extend them (different model such as ChatGPT or GPT4, prompts, decoding parameters), refer to the [Configuring annotators](#Configuring-annotators) section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d6dcf4-b0fb-4c56-a1fd-1db509af05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_farm.utils import jload\n",
    "from alpaca_farm.auto_annotations import PairwiseAutoAnnotator\n",
    "\n",
    "annotator = PairwiseAutoAnnotator(annotators_config=\"annotators/test/configs.yaml\", **decoding_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99ac04-0d48-410e-95fd-cf3b1360fc4e",
   "metadata": {},
   "source": [
    "## Annotating paired outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef99a2c-77ef-41e6-9db8-eae091afd952",
   "metadata": {},
   "source": [
    "Now let's annotate some pairwise preference. All we need is some data. The annotator takes in either list of dictionaries, or pandas dataframes. The keys of dictionaries (or columns in dataframe) need to always contain `instruction` and `input` which defines the instruction. The annotator also needs a pair of outputs to compare, if you have a sequence of such pairs under the keys `output_1` and `output_2` then you can directly use `annotate_pairs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab42c1d-3bda-4797-a1bd-877b958e8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of paired output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Hey everyone! \\n\\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \\n\\nLet me know if you can make it - I'd love to see you all there!\\n\\nCheers,\\n[Your Name]\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_pairs = jload(\"examples/data/outputs_pairs.json\")[:6]\n",
    "print(\"Example of paired output:\\n\")\n",
    "outputs_pairs[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67caa934-5a37-429a-affc-dbd182be7c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 6 examples with davinci003_3\n",
      "INFO:root:Auto annotating 2 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 0.0}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_pairs(outputs_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd953df-7eb1-4446-86a9-f3b3fbfba665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': \"Hey everyone! \\n\\nI'm hosting a dinner party this Friday night and I'd love for all of you to come over. We'll have a delicious spread of food and some great conversations. \\n\\nLet me know if you can make it - I'd love to see you all there!\\n\\nCheers,\\n[Your Name]\",\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 2}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648a4b0-5d19-4bc7-b63e-d4cb4ab4fd51",
   "metadata": {},
   "source": [
    "Here we see that the annotations adds two keys:\n",
    "- `'preference'`: the index of the preferred output, here `preference=1.0` so `output_1` is prefered\n",
    "- `'annotator'`: the name of the simulated annotator as found in the `annotators_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ee339-cf40-4a3c-a861-19c265f962bc",
   "metadata": {},
   "source": [
    "`annotate_pairs` is the main function and should be used if you have paired outputs to annotate. In many usecases, however, you  will outputs in different formats. In the following we discuss two additional helper function `annotate_head2head` and `annotate_samples` which are paticularly well suited for the typical format during evaluation and training respectively. Both functions call `annotate_pairs` under the hood after a reformatting step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b41af4-731c-4d12-83b4-9b69f4b2bc3f",
   "metadata": {},
   "source": [
    "## Evaluation through pairwise comparison\n",
    "For evaluation we need two components:\n",
    "- outputs from the model we want to compare\n",
    "- outputs on the same examples from the baseline model\n",
    "\n",
    "Often case both of those components will be in a different list of dictionary (one list for each model). In this case all dictionaries need to contain an `output` column. Let us load such data from our simulated RLHF model and text-davinci-003 baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a9cce3-0fe9-4dc2-9b49-de59f641e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of baseline output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_baseline = jload(\"examples/data/outputs_baseline.json\")[:6]\n",
    "print(\"Example of baseline output:\\n\")\n",
    "outputs_baseline[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f808315-da5c-4af3-a026-64e3d07801f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of rlhf output:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output': 'Dear Friends, \\n\\nI am writing to invite you all to a dinner on Friday evening. It is a casual affair, and I am looking forward to a fun evening catching up with you all. I am planning to make a selection of delicious dishes, ranging from appetizers to mains and desserts. There will be something for everyone to enjoy, and I am sure it will be a night to remember.\\n\\nThe dinner will be held at my place on Friday, April 17th at 7pm. If you are interested in joining me, please RSVP to this email by Thursday, April 16th. I am looking forward to seeing you all there! \\n\\nThank you, \\n\\n[Name]'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_rlhf = jload(\"examples/data/outputs_rlhf.json\")[:6]\n",
    "print(\"Example of rlhf output:\\n\")\n",
    "outputs_rlhf[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ff9ed-e67b-41df-b706-f2633ec9b909",
   "metadata": {},
   "source": [
    "The annotator's function of interest when we have two sequences of outputs is `annotate_head2head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e613e0ca-2db8-436c-a4c7-bf71de116ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 6 examples with davinci003_3\n",
      "INFO:root:Auto annotating 2 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 0.0}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72e4e7a-cd40-41c3-87b7-995e323e35d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': 'Dear Friends, \\n\\nI am writing to invite you all to a dinner on Friday evening. It is a casual affair, and I am looking forward to a fun evening catching up with you all. I am planning to make a selection of delicious dishes, ranging from appetizers to mains and desserts. There will be something for everyone to enjoy, and I am sure it will be a night to remember.\\n\\nThe dinner will be held at my place on Friday, April 17th at 7pm. If you are interested in joining me, please RSVP to this email by Thursday, April 16th. I am looking forward to seeing you all there! \\n\\nThank you, \\n\\n[Name]',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1699e-eef3-4c98-bc83-512f5f401c6f",
   "metadata": {},
   "source": [
    "We see that the format of the output is the same as before. Here `preference` indicates that the simulator prefered the output `output_2`, which corresponds to our RLHF model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6a404-4fc1-4f0c-9858-02f24a189b95",
   "metadata": {},
   "source": [
    "## Training\n",
    "For training we typically have multiple outputs for each instruction which are sampled from the same (SFT) model as seen in the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50354b51-12c5-4833-81af-549d78ef3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of different sampled outputs from SFT:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Describe a time when you had to make a difficult decision.',\n",
       "  'input': '',\n",
       "  'output': [\"I had to make a difficult decision a few years ago when I was offered a job that would require me to move to a different city. I had to weigh the pros and cons of this job offer, and consider what it would mean for me to leave my friends and family in order to take it. In the end, I decided to turn down the job offer since I wasn't ready to quit my current job, move to a different city, and leave my loved ones behind.\",\n",
       "   'I had to make a difficult decision last year when I was faced with a life-changing opportunity. I had to decide whether or not to leave my current job and move to a different city for the chance to further my career. After much consideration, I chose to take the risk and make the move. Thankfully, it paid off and I am now enjoying my new job and exploring the city.',\n",
       "   \"I had to make a difficult decision when I was offered a job in a city that was significantly farther away from my friends and family. I weighed the pros and cons and consulted with people I trusted, but in the end I had to make the choice that was best for my future and my well-being. It was a difficult decision, but I'm glad I made the choice that allowed me to further my career and start my journey towards independence.\",\n",
       "   'I remember a time when I had to decide whether to stay in my hometown or move to a new city. It was a difficult decision because I had grown up in the town, and I was used to the familiarity of my childhood home. On the other hand, the new city had more job opportunities and a better environment that I thought would be beneficial for my future. After weighing the pros and cons, I made the decision to move to the new city. It was the right choice, as I was able to find a job I enjoyed and make some great friends in the new place.']}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_samples = jload(\"examples/data/multisamples_sft.json\")[:1]\n",
    "print(\"Example of different sampled outputs from SFT:\\n\")\n",
    "outputs_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df755cb-8ff5-478d-b397-682ab3d745e1",
   "metadata": {},
   "source": [
    "In this case, you can use the annotator's `annotate_samples`, which first samples pairs of outputs that have the same instruction/input and then annotate those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8557da-46fd-4906-a209-a02cc12eef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Filtered unique instruction/input pairs: 4 -> 1\n",
      "INFO:root:Annotating 1 examples with davinci003_3\n",
      "INFO:root:Auto annotating 1 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 0.0}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_samples(outputs_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a55348a-a0ca-4fe1-b6d1-b9fa5d1c339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_id': 0,\n",
       "  'instruction': 'Describe a time when you had to make a difficult decision.',\n",
       "  'input': '',\n",
       "  'output_1': \"I had to make a difficult decision a few years ago when I was offered a job that would require me to move to a different city. I had to weigh the pros and cons of this job offer, and consider what it would mean for me to leave my friends and family in order to take it. In the end, I decided to turn down the job offer since I wasn't ready to quit my current job, move to a different city, and leave my loved ones behind.\",\n",
       "  'output_2': 'I had to make a difficult decision last year when I was faced with a life-changing opportunity. I had to decide whether or not to leave my current job and move to a different city for the chance to further my career. After much consideration, I chose to take the risk and make the move. Thankfully, it paid off and I am now enjoying my new job and exploring the city.',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77b705-1845-4994-bd98-17ecfbff3d7f",
   "metadata": {},
   "source": [
    "By default there will only be one pair per instruction,input. If you use `is_unique_instructions=False` then you will get as many pairs as outputs.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515669f8-622c-47f9-9997-c0dedeb797c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Going further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec356bc8-82cc-4ab0-aa05-b2f3bbce3408",
   "metadata": {},
   "source": [
    "### Adding noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403b9a8-b444-4861-a837-68fc869dfde9",
   "metadata": {},
   "source": [
    "During training we typically flip the the label with probability 0.25 to emulate the variability of human annotations. To do so you can either:\n",
    "- initialize the annotator with `PairwiseAutoAnnotator(p_label_flip=0.25)`\n",
    "- set the noise of an initialized annotator `annotator.set_noise(p_label_flip=0.25)` \n",
    "- give the noise to annotate_samples `annotate_samples(..., p_label_flip=0.25)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68702d25-3a12-4de6-bef8-e21e79c8d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Filtered unique instruction/input pairs: 4 -> 1\n",
      "INFO:root:Adding random noise to the labels p_label_flip=0.25.\n",
      "INFO:root:Annotating 0 examples with davinci003_3\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_samples(outputs_samples, p_label_flip=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d663d65-5da5-40b5-a14b-f2f925649132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sample_id': 0,\n",
       "  'instruction': 'Describe a time when you had to make a difficult decision.',\n",
       "  'input': '',\n",
       "  'output_1': \"I had to make a difficult decision a few years ago when I was offered a job that would require me to move to a different city. I had to weigh the pros and cons of this job offer, and consider what it would mean for me to leave my friends and family in order to take it. In the end, I decided to turn down the job offer since I wasn't ready to quit my current job, move to a different city, and leave my loved ones behind.\",\n",
       "  'output_2': 'I had to make a difficult decision last year when I was faced with a life-changing opportunity. I had to decide whether or not to leave my current job and move to a different city for the chance to further my career. After much consideration, I chose to take the risk and make the move. Thankfully, it paid off and I am now enjoying my new job and exploring the city.',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a596a8-0941-4e19-b19a-ee03dad7c1e1",
   "metadata": {},
   "source": [
    "### Cost & time efficiency: avoiding duplicate annotation\n",
    "Often time you will find yourself reannotating examples that you already annotated. This is particularly true if you sample many times from the same model (eg to get pairwise preferences for training or to evaluate best-of-n) or if the instructions you are considering require short outputs => many models might give the same exact answer.\n",
    "\n",
    "This means that you have to spend unecessary money and time. Thankfully `PairwiseAutoAnnotator` stores previous annotations and will reuse those. For example, let us reannotate the previous evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58d43c91-0ef4-417e-bbb2-ce585f6c35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 0 examples with davinci003_3\n"
     ]
    }
   ],
   "source": [
    "annotated = annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd5e9051-23fe-4294-9993-0c7afbc74495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': 'Dear Friends, \\n\\nI am writing to invite you all to a dinner on Friday evening. It is a casual affair, and I am looking forward to a fun evening catching up with you all. I am planning to make a selection of delicious dishes, ranging from appetizers to mains and desserts. There will be something for everyone to enjoy, and I am sure it will be a night to remember.\\n\\nThe dinner will be held at my place on Friday, April 17th at 7pm. If you are interested in joining me, please RSVP to this email by Thursday, April 16th. I am looking forward to seeing you all there! \\n\\nThank you, \\n\\n[Name]',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84836c18-9f26-4b1f-8660-7f2fa39817ee",
   "metadata": {},
   "source": [
    "We now get the annotations without actually having to reannotate any example.\n",
    "By default, this the previous annotations are only stored in memory. If you want those to be stateful, you need to either give `saving_path` to the constructor of `PairwiseAutoAnnotator` (automatically save and loads) or manually call `save` and `load`.\n",
    "Here's an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed7c4e90-0648-41ef-b903-71dc0a3f627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saving all annotations to examples/data/annotations.json.\n",
      "INFO:root:Loading all annotations from examples/data/annotations.json.\n"
     ]
    }
   ],
   "source": [
    "saving_path = \"examples/data/annotations.json\"\n",
    "annotator.save(saving_path) # manually save the previous annotation\n",
    "new_annotator = PairwiseAutoAnnotator(annotators_config=\"annotators/test/configs.yaml\",\n",
    "                                     saving_path=saving_path # will automatically load and save future annotations\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da900044-8e27-4006-b820-20b3a365c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 0 examples with davinci003_3\n",
      "INFO:root:Saving all annotations to examples/data/annotations.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'If you could help me write an email to my friends inviting them to dinner on Friday, it would be greatly appreciated.',\n",
       "  'input': '',\n",
       "  'output_1': \"Dear Friends, \\r\\n\\r\\nI hope this message finds you well. I'm excited to invite you to dinner on Friday. We'll meet at 7:00 PM at [location]. I look forward to seeing you there. \\r\\n\\r\\nBest,\\r\\n[Name]\",\n",
       "  'output_2': 'Dear Friends, \\n\\nI am writing to invite you all to a dinner on Friday evening. It is a casual affair, and I am looking forward to a fun evening catching up with you all. I am planning to make a selection of delicious dishes, ranging from appetizers to mains and desserts. There will be something for everyone to enjoy, and I am sure it will be a night to remember.\\n\\nThe dinner will be held at my place on Friday, April 17th at 7pm. If you are interested in joining me, please RSVP to this email by Thursday, April 16th. I am looking forward to seeing you all there! \\n\\nThank you, \\n\\n[Name]',\n",
       "  'annotator': 'davinci003_3',\n",
       "  'preference': 1}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated = new_annotator.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)\n",
    "annotated[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea60a22-a387-4309-8d1b-b78bdcac5a5a",
   "metadata": {},
   "source": [
    "We suggest that you simply set `saving_path=...` when initializing the annotator and you will never have to deal with manual saving and loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb89a3-7200-4f58-a7f5-9175f36d7cad",
   "metadata": {},
   "source": [
    "### Evaluating win rates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78922f60-a9aa-4383-b18d-c95a93c6546c",
   "metadata": {},
   "source": [
    "Let's show how to get win rates from annotations using our actual pool of annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "826421f1-e644-42c9-9375-d8b08fb958ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_farm.auto_annotations.analysis import head2head_to_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93ffaa6c-9570-4384-b157-d6f95b6d90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the following is selfinstruct eval\n",
    "outputs_baseline = jload(\"examples/data/outputs_baseline.json\")\n",
    "outputs_rlhf = jload(\"examples/data/outputs_rlhf.json\")\n",
    "outputs_sft = jload(\"examples/data/outputs_sft.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e000d42-6c37-40c6-81ac-eef3d34a4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_pool = PairwiseAutoAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b79ba1-aeb3-4dd0-8669-a171ba4a2c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 18 examples with gpt4_1\n",
      "INFO:root:Auto annotating 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 600, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches:   0%|                                                                                                                          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "annotated_sft = annotator_pool.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_sft)\n",
    "head2head_to_metrics(preferences=[a[\"preference\"] for a in annotated_sft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c05eb80-0f24-4570-86ea-40da1a2a95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Annotating 20 examples with gpt4_1\n",
      "INFO:root:Auto annotating 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 600, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:47<00:00,  9.44s/it]\n",
      "INFO:root:Annotating 23 examples with gpt4_2\n",
      "INFO:root:Auto annotating 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:22<00:00,  4.49s/it]\n",
      "INFO:root:Annotating 28 examples with gpt4_3\n",
      "INFO:root:Auto annotating 7 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:38<00:00,  5.50s/it]\n",
      "INFO:root:Annotating 9 examples with gpt4_4\n",
      "INFO:root:Auto annotating 9 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:04<00:00,  2.15it/s]\n",
      "INFO:root:Annotating 18 examples with gpt4_5\n",
      "INFO:root:Auto annotating 5 prompts using gpt-4-0314.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 250, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model gpt-4-0314 and numproc == 5.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.14s/it]\n",
      "INFO:root:Annotating 21 examples with chatgpt_1\n",
      "INFO:root:Auto annotating 21 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 50, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:03<00:00,  5.95it/s]\n",
      "INFO:root:Annotating 16 examples with chatgpt_2\n",
      "INFO:root:Auto annotating 16 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 150, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches:  38%|██████████████████████████████████████████▍                                                                      | 6/16 [00:12<00:16,  1.67s/it]WARNING:root:OpenAIError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00daa949319d09d793ac5cd657cc88ab in your message.).\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "WARNING:root:Switching to organization: org-dPb4AamVADaroNRyV4QK3Yyt for OAI API key.\n",
      "WARNING:root:OpenAIError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ed770c4273969103977ff6b060c3f279 in your message.).\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "WARNING:root:Switching to organization: org-aGS1aYCpHCgg5HGJjdcvj1Gi for OAI API key.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [03:40<00:00, 13.75s/it]\n",
      "INFO:root:Annotating 26 examples with chatgpt_3\n",
      "INFO:root:Auto annotating 26 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches:  38%|███████████████████████████████████████████                                                                     | 10/26 [00:20<00:03,  5.15it/s]WARNING:root:OpenAIError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8e90d7451ba45f0986a2edcad206db6a in your message.).\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "WARNING:root:Switching to organization: org-aGS1aYCpHCgg5HGJjdcvj1Gi for OAI API key.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:34<00:00,  1.32s/it]\n",
      "INFO:root:Annotating 19 examples with chatgpt_4\n",
      "INFO:root:Auto annotating 19 prompts using gpt-3.5-turbo-0301.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 20, 'temperature': 1.0, 'logit_bias': {21279: -100, 63295: -100, 4155: -100, 11995: -100, 25215: -100, 50344: -100, 59047: -100, 2196: -100, 2181: -100, 21704: -100, 19701: -100, 5207: 7, 320: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model gpt-3.5-turbo-0301 and numproc == 5.\n",
      "prompt_batches:  32%|███████████████████████████████████▋                                                                             | 6/19 [00:01<00:02,  5.62it/s]WARNING:root:OpenAIError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b144913304aaa1ae18283839ca75868c in your message.).\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "WARNING:root:Switching to organization: org-dPb4AamVADaroNRyV4QK3Yyt for OAI API key.\n",
      "WARNING:root:OpenAIError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 298117935f3b5b5ac1c29bdaa1ec893f in your message.).\n",
      "WARNING:root:Hit request rate limit; retrying...\n",
      "WARNING:root:Switching to organization: org-aGS1aYCpHCgg5HGJjdcvj1Gi for OAI API key.\n",
      "prompt_batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [01:05<00:00,  3.46s/it]\n",
      "INFO:root:Annotating 19 examples with davinci003_1\n",
      "INFO:root:Auto annotating 6 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0, 'logit_bias': {7: 7, 64: 7, 8: 7, 65: 7}}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.98s/it]\n",
      "INFO:root:Annotating 14 examples with davinci003_2\n",
      "INFO:root:Auto annotating 14 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.16it/s]\n",
      "INFO:root:Annotating 16 examples with davinci003_3\n",
      "INFO:root:Auto annotating 4 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "INFO:root:Annotating 20 examples with davinci003_4\n",
      "INFO:root:Auto annotating 20 prompts using text-davinci-003.\n",
      "INFO:root:Kwargs to completion: {'max_tokens': 200, 'temperature': 1.0}\n",
      "INFO:root:Decoding with OpenAI API model text-davinci-003 and numproc == 1.\n",
      "prompt_batches: 100%|██████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.65s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'win_rate': 0.49007936507936506,\n",
       " 'standard_error': 0.03149081428773814,\n",
       " 'n_wins': 123,\n",
       " 'n_wins_base': 128,\n",
       " 'n_draws': 1,\n",
       " 'n_total': 252}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_rlhf = annotator_pool.annotate_head2head(outputs_1=outputs_baseline, outputs_2=outputs_rlhf)\n",
    "head2head_to_metrics(preferences=[a[\"preference\"] for a in annotated_rlhf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703c1ea-8406-4599-8a43-d5a5de63459e",
   "metadata": {},
   "source": [
    "### Configuring annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4927636c-96f5-4125-a553-a5be011c2a42",
   "metadata": {},
   "source": [
    "The most important argument to `PairwiseAutoAnnotator` is `annotators_config` which defines the pool of annotators (the API provider, the model, the prompt, and the decoding parameters) . We provide the following options out of the box:\n",
    "- `annotators_config=\"annotators/annotator_pool_v0/configs.yaml\"`: ApacaFarm's default annotator pool\n",
    "- `annotators_config=\"annotators/greedy_gpt4/configs.yaml\"`: Greedy GPT4 annotator\n",
    "- `annotators_config=\"annotators/test/configs.yaml\"`: a faster text-davinci-003 annotator useful for testing\n",
    "\n",
    "Here's the desciption of `annotators_config` from the docstring:\n",
    "```\n",
    "A dictionary or path to a yaml file containing the configuration for the pool of annotators. The keys in the first dictionary should be the annotator's name, and the value should be a dictionary of the annotator's configuration which should have the following keys:\n",
    "\n",
    "- prompt_templates (dict): a dictionary of prompt templates or path to the prompts. The keys should be \"without_inputs\" and \"with_inputs\". Each template should contain placeholders for keys in the example dictionary, typically {instruction} and {output_1} {output_2}.\n",
    "- fn_decoder (str): function in `alpaca_farm.auto_annotations.pairwise_annotators.decoders.py` for completions.\n",
    "- decoder_kwargs (dict): kwargs for fn_decode. E.g. model_name, max_tokens, temperature, tokens_to_avoid, tokens_to_favor\n",
    "- outputs_to_match (dict): a dictionary of outputs to match from the completions. The values should be a regex pattern that should be matched, the keys should be the corresponding preference value. For example {1: 'Output \\(a\\)'} will match the output \"Output (a)\" and set the preference to 1.\n",
    "- other kwargs to `SinglePairwiseAutoAnnotator` such as batch_size\n",
    "```\n",
    "\n",
    "And here is config `\"annotators/test/configs.yaml\"` of the annotator we used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "945ba06f-cc13-436f-83f0-f074f37ea2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davinci003_3 : # text-davinci-003_v1_b5-pairwise_temp=1.0\n",
      "  prompt_templates:\n",
      "    with_inputs: \"annotators/annotator_pool_v0/text_b5_with_inputs.txt\"\n",
      "    without_inputs: \"annotators/annotator_pool_v0/text_b5_without_inputs.txt\"\n",
      "  fn_decoder: \"openai_completions\"\n",
      "  decoder_kwargs:\n",
      "    model_name: \"text-davinci-003\"\n",
      "    max_tokens: 200\n",
      "    temperature: 0.0\n",
      "  outputs_to_match:\n",
      "    1: '\\n\\(a\\)'\n",
      "    2: '\\n\\(b\\)'\n",
      "  batch_size: 5"
     ]
    }
   ],
   "source": [
    "!cat src/alpaca_farm/auto_annotations/annotators/test/configs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233e875-ae40-4b53-bc6b-05cf0201bbf0",
   "metadata": {},
   "source": [
    "For more information see the docstrings of the following classes (eg uncomment and run the following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "459aa856-241b-451b-8917-da24b6f85052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from alpaca_farm.auto_annotations.pairwise_annotators import SinglePairwiseAutoAnnotator\n",
    "# PairwiseAutoAnnotator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730ed569-d910-434c-a5d9-6600cab17701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SinglePairwiseAutoAnnotator?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d886bf-3002-4939-8751-332cf4ef0e35",
   "metadata": {},
   "source": [
    "If you want to make a new annotator, change the prompt and or the configs above. See `annotators/annotator_pool_v0/...txt` for the actual prompts we used. To see the prompt used above (which is part of our pool of prompts) uncomment and run the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb96d79-e4e6-407c-b7e0-758ec75bc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat src/alpaca_farm/auto_annotations/annotators/annotator_pool_v0/text_b5_without_inputs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd755d-1d53-4c9c-b687-3b0389f289e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
